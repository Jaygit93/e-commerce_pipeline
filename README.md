# E-commerce Data Pipeline with Airflow

## üì¶ Description

Ce projet met en place un pipeline de traitement automatis√© de donn√©es e-commerce √† l‚Äôaide d‚ÄôAirflow.

Les donn√©es proviennent du fichier **"Online Retail II"** publi√© sur le [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II). Elles contiennent des transactions B2C effectu√©es par un distributeur britannique entre 2010 et 2011, incluant des colonnes telles que :
- InvoiceNo
- StockCode
- Description
- Quantity
- Price (remplac√© ici par UnitPrice)
- CustomerID
- Country

## ‚öôÔ∏è Architecture Docker

Le projet repose sur une stack Docker avec les services suivants :
- `airflow`: Webserver Airflow (port 8090)
- `airflow_scheduler`: Scheduler d√©di√©
- `mysql`: Base de donn√©es (port expos√© : 3307)
- `phpmyadmin`: Interface de gestion MySQL (port expos√© : 8081)
- `redis`: Service de cache utilis√© par Airflow

## üìÇ Pipeline Airflow

1. **Conversion des donn√©es** : le fichier Excel est converti en CSV via `data/download.py`
2. **Chargement dans MySQL** : les donn√©es nettoy√©es (gestion des NaN via `safe_val`) sont ins√©r√©es dans la base `data_airflow`, table `sales` via un DAG Airflow quotidien planifi√© √† 6h.
3. **Connexion s√©curis√©e** : l‚Äôacc√®s √† MySQL est configur√© dynamiquement √† l‚Äôaide d‚Äôun hook Airflow (`BaseHook.get_connection('mysql_conn')`), pour √©viter de stocker les identifiants en dur.

## üêç Exemple de script Python

Fichier : `dags/ecommerce_data_pipeline.py`

- Lit le fichier `/opt/airflow/data/online_retail.csv`
- Nettoie chaque ligne (gestion des valeurs manquantes)
- Ins√®re les donn√©es dans MySQL via un `PythonOperator`

## üì¶ docker-compose.yml

Le fichier `docker-compose.yml` d√©finit la configuration compl√®te, notamment :
- Le montage des volumes (`dags`, `data`, `logs`, `config`)
- L'installation des d√©pendances via `requirements.txt`
- Le d√©marrage conditionnel bas√© sur la disponibilit√© de MySQL

## ‚ñ∂Ô∏è Lancement

1. Cloner ce d√©p√¥t
2. Convertir le fichier Excel : `python data/download.py`
3. Lancer la stack : `docker-compose up -d`
4. Acc√©der √† l'interface Airflow sur [http://localhost:8090](http://localhost:8090)
5. Lancer manuellement le DAG `ecommerce_data_pipeline`

## üõ† Requis
- Docker & Docker Compose
- Le fichier source Excel `online_retail_II.xlsx`

## üßæ License

Ce projet est un projet d‚Äôapprentissage bas√© sur un dataset public.